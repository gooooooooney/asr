# VAD-Based Streaming ASR 技术报告

## 1. 项目概述

### 1.1 项目背景
本项目旨在构建一个基于VAD（Voice Activity Detection）的流式ASR（Automatic Speech Recognition）系统，通过WebSocket实现网页端与服务端的实时音频传输和处理。系统结合TEN-VAD进行语音活动检测，使用Whisper进行语音识别，并可选配LLM进行文本修正，提供高质量的实时语音转文字服务。

### 1.2 项目目标
- 实现低延迟的实时语音识别系统
- 基于VAD智能切分音频片段，优化识别效果
- 支持长时间持续识别，处理超长音频流
- 提供可选的LLM文本修正功能，提升识别质量
- 展示ASR原始结果、LLM修正结果及处理时延

### 1.3 技术栈
- **前端**: HTML5, JavaScript, WebSocket API, Web Audio API
- **后端**: Node.js/TypeScript, WebSocket (ws库)
- **VAD**: TEN-VAD (WebAssembly/Node.js版本)
- **ASR**: Whisper V3 (via Fireworks API)
- **LLM**: 可配置的LLM服务（用于文本修正）

## 2. 产品需求文档（PRD）

### 2.1 功能需求

#### 2.1.1 核心功能
1. **实时音频流传输**
   - 网页端通过WebSocket实时发送音频数据到服务端
   - 支持16kHz采样率的音频流
   - 低延迟传输（目标<100ms）

2. **智能语音活动检测**
   - 使用TEN-VAD检测语音/静音状态
   - 自动切分语音片段
   - 支持配置静音阈值和持续时间

3. **分段ASR识别**
   - 基于VAD结果智能切分音频
   - 使用前文内容作为prompt提升准确率
   - 支持超长音频的分块处理

4. **LLM文本修正（可选）**
   - 对ASR结果进行语言规范化
   - 添加标点符号
   - 修正常见识别错误

5. **实时结果展示**
   - 显示原始ASR结果
   - 显示LLM修正后的结果（如启用）
   - 显示处理时延统计

#### 2.1.2 非功能需求
1. **性能要求**
   - 端到端延迟 < 2秒
   - 支持并发用户数 > 10
   - CPU使用率 < 50%

2. **可靠性要求**
   - WebSocket断线自动重连
   - ASR服务失败自动重试
   - 优雅的错误处理和提示

3. **安全要求**
   - API密钥安全传输和存储
   - 音频数据加密传输（WSS）
   - 用户隐私保护

### 2.2 用户界面设计

#### 2.2.1 网页端界面
```
┌─────────────────────────────────────────┐
│          VAD-Based Streaming ASR         │
├─────────────────────────────────────────┤
│ API Key: [________________] [Connect]   │
├─────────────────────────────────────────┤
│ ┌─────────────────────────────────────┐ │
│ │ ASR原始结果:                        │ │
│ │ [实时显示区域]                      │ │
│ │                                     │ │
│ └─────────────────────────────────────┘ │
│ ┌─────────────────────────────────────┐ │
│ │ LLM修正结果:                        │ │
│ │ [实时显示区域]                      │ │
│ │                                     │ │
│ └─────────────────────────────────────┘ │
├─────────────────────────────────────────┤
│ 状态: [连接中] | 延迟: [150ms]          │
│ □ 启用LLM文本修正                       │
└─────────────────────────────────────────┘
```

### 2.3 系统架构设计

#### 2.3.1 整体架构
```
┌─────────────┐     WebSocket      ┌─────────────┐
│   网页端    │ ←─────────────────→ │   服务端    │
│             │                     │             │
│ - 音频采集  │     Audio Stream    │ - WebSocket │
│ - WebSocket │ ─────────────────→  │ - TEN-VAD   │
│ - UI展示    │                     │ - Whisper   │
│             │ ←─────────────────  │ - LLM修正   │
└─────────────┘     ASR Results     └─────────────┘
```

#### 2.3.2 数据流设计
```
音频输入 → 网页端采集 → WebSocket传输 → 服务端接收
                                           ↓
                                      TEN-VAD检测
                                           ↓
                                    [有语音→无语音]
                                           ↓
                                    提取音频片段
                                           ↓
                                    Whisper ASR
                                    (带前文prompt)
                                           ↓
                                    [可选] LLM修正
                                           ↓
                                    返回结果到网页端
```

### 2.4 核心算法逻辑

#### 2.4.1 VAD切分逻辑
1. **基础切分**：当检测到从"有语音"状态切换到"无语音"状态时，将该段音频发送给Whisper

2. **超时切分**：如果连续X秒（如3秒）未检测到静音，强制切分并发送
   - 每3秒生成一个ASR结果，立即展示在界面上
   - 持续说话时，每3秒都会产生新的识别结果
   - 只提取实际语音部分，不包含之前的静音

3. **循环处理**：对于超长音频，持续以X秒为单位创建chunk进行处理
   - 例如：10秒连续语音会产生3个3秒chunk，外加1个1秒的chunk

4. **中断重识别**：当语音结束（VAD检测到静音）时
   - **短语音（≤9秒）**：重识别整个语音段
     - 例：4秒语音，重识别0-4秒
   - **长语音（>9秒）**：只重识别最后9秒
     - 计算往回9秒的位置
     - 找到该位置右侧最近的chunk边界
     - 从该边界到结束进行重识别
     - 例1：12秒语音结束，往回9秒到3秒位置，找到右侧最近chunk边界4秒，重识别4-12秒
     - 例2：12.5秒语音结束，往回9秒到3.5秒位置，找到右侧最近chunk边界7秒，重识别7-12.5秒
   - **结果覆盖**：新的识别结果覆盖对应时间段的旧chunk结果

#### 2.4.2 Prompt优化策略

1. **基本原则**：使用前文内容作为prompt，但不包括即将被重识别覆盖的部分

2. **具体规则**：
   - **首次识别**：使用之前已完成的最后2个识别结果作为prompt
   - **重识别时**：
     - 短语音（≤9秒）：使用该语音段之前的最后2个结果
     - 长语音（>9秒）：使用重识别起始点之前的最后2个结果
   
3. **示例说明**：
   ```
   已识别：["你好"(0-3s), "今天天气"(3-6s), "真不错"(6-9s), "我想去"(9-12s)]
   
   场景1：新增3秒chunk (12-15s)
   - Prompt: "真不错 我想去"（最后2个结果）
   
   场景2：15秒结束，重识别6-15秒
   - 要覆盖："真不错"(6-9s), "我想去"(9-12s) 
   - Prompt: "你好 今天天气"（6秒之前的最后2个）
   
   场景3：4秒结束，重识别0-4秒
   - 要覆盖："你好"(0-3s)
   - Prompt: ""（没有之前的结果）
   ```

4. **优化效果**：
   - 保持上下文连贯性
   - 避免循环依赖（不用即将被覆盖的内容做prompt）
   - 提高识别准确率

#### 2.4.3 LLM文本修正Prompt
```
角色：
你是一名专业的ASR（自动语音识别）后处理专家。

核心原则：
1. 专注修正，而非改写：你的唯一任务是修正ASR系统因发音相似、口音、语速过快等因素导致的识别错误。对于原文中语义通顺、语法正确的部分，必须保持原样，不得进行任何同义词替换、语序调整或句式改写。
2. 上下文感知与逻辑判断：当一个词语在当前语境下显得突兀、不合逻辑时（例如，在技术讨论中出现一个毫不相关的日常词汇），应优先判断它是一个由发音相似词语造成的识别错误。
3. 保留原始结构：修正应在最小范围内进行，仅替换被错误识别的词语，并完整保留原始句子的结构和所有正确的词汇。

任务：
请对以下ASR识别结果进行修正。

输入文本：{asr_result}

具体要求：
1. 识别并修正同音字错误（保持韵母一致）
2. 修正因口音或语速导致的识别错误
3. 添加合适的标点符号（。，！？、等）
4. 修正大小写（英文）和语种混用问题
5. 规范数字、日期、时间的表达方式
6. 保持原始句子结构，不进行句式改写

操作流程：
1. 生成三个修正备选句：
   - 基于上述核心原则，识别出原始文本中的可疑错误词语
   - 生成三个仅在错误词语修正上有所不同的候选句子
   - 如果只有一个明显的正确答案，可以围绕该答案提供微小但合理的变体（例如，标点符号或个别语气词的差异）
   - 关键：这三个句子必须保持与原文一致的句子结构

2. 选择最佳句子：
   - 从生成的三个备选句中，选出最符合逻辑、最能还原说话者原始意图的一句

输出格式：
{
  "candidate1": "修正方案1",
  "candidate2": "修正方案2", 
  "candidate3": "修正方案3",
  "best": "最佳选择",
  "reason": "选择理由"
}
```

## 3. 技术实现方案

### 3.1 LLM文本修正实现

使用Fireworks API的Kimi K2 Instruct模型进行ASR文本修正：

#### 3.1.1 API配置
```python
class LLMProcessor:
    def __init__(self, api_key):
        self.api_key = api_key
        self.api_url = "https://api.fireworks.ai/inference/v1/chat/completions"
        self.model = "accounts/fireworks/models/kimi-k2-instruct"
```

#### 3.1.2 处理流程
1. **构建专业prompt**：包含ASR后处理专家角色设定和详细要求
2. **生成3个候选修正**：识别可疑错误词语，生成仅在修正上不同的候选句
3. **选择最佳结果**：从候选中选出最符合逻辑的修正
4. **错误处理**：
   - API调用失败：返回原文本+标点
   - JSON解析失败：尝试多种字段名
   - 超时保护：30秒超时限制
   - 异常降级：确保系统稳定性

#### 3.1.3 API参数设置
- temperature: 0.6（平衡准确性和创造性）
- max_tokens: 4096
- top_p: 1, top_k: 40
- presence_penalty: 0, frequency_penalty: 0

### 3.2 WebSocket通信协议

#### 3.2.1 消息格式定义
```typescript
// 客户端 → 服务端
interface ClientMessage {
  type: 'audio' | 'config' | 'control';
  data: {
    // audio类型
    audioData?: ArrayBuffer;  // 16kHz PCM音频数据
    
    // config类型
    apiKey?: string;
    enableLLM?: boolean;
    vadConfig?: {
      silenceThreshold?: number;  // 静音阈值
      silenceDuration?: number;   // 静音持续时间(ms)
    };
    
    // control类型
    command?: 'start' | 'stop' | 'reset';
  };
  timestamp: number;
}

// 服务端 → 客户端
interface ServerMessage {
  type: 'result' | 'status' | 'error';
  data: {
    // result类型
    asrResult?: string;
    llmResult?: string;
    segmentId?: number;
    isFinal?: boolean;
    isTimeoutChunk?: boolean;      // 是否为超时切分的chunk
    isReprocessed?: boolean;       // 是否为重新识别的结果
    replacesSegments?: number[];   // 需要覆盖的segment IDs
    processingTime?: number;
    
    // status类型
    status?: 'connected' | 'processing' | 'ready';
    vadState?: 'speech' | 'silence';
    
    // error类型
    error?: string;
    code?: string;
  };
  timestamp: number;
}
```

### 3.2 服务端实现

#### 3.2.1 主要模块
1. **WebSocket服务器**：处理连接、消息收发
2. **音频缓冲管理**：管理音频流和分段
3. **VAD处理器**：集成TEN-VAD进行语音检测
4. **ASR调用器**：调用Whisper API
5. **LLM处理器**：可选的文本修正
6. **结果管理器**：管理识别历史和prompt

#### 3.2.2 核心流程
```javascript
// 伪代码示例
class StreamingASRServer {
  constructor() {
    this.audioBuffer = new AudioBuffer();
    this.vadProcessor = new TenVADProcessor();
    this.asrClient = new WhisperClient();
    this.previousResults = [];  // 用于prompt
  }
  
  async processAudioChunk(audioData) {
    // 1. 添加到缓冲区
    this.audioBuffer.append(audioData);
    
    // 2. VAD检测
    const vadResult = await this.vadProcessor.process(audioData);
    
    // 3. 根据VAD状态处理
    if (vadResult.stateChanged && vadResult.currentState === 'silence') {
      // 检查是否在超时切分过程中遇到VAD
      if (this.recentChunks.length > 0) {
        // 中断重识别：收集最近的chunks进行重新识别
        await this.reprocessRecentChunks();
      } else {
        // 普通VAD切分
        const segment = this.audioBuffer.extractSegment();
        await this.processSegment(segment, false);
      }
    }
    
    // 4. 检查超时
    if (this.audioBuffer.duration > MAX_SEGMENT_DURATION) {
      const segment = this.audioBuffer.extractFixedDuration(MAX_SEGMENT_DURATION);
      const chunkId = await this.processSegment(segment, true);
      
      // 记录chunk信息，用于可能的重识别
      this.recentChunks.push({
        id: chunkId,
        audioData: segment,
        timestamp: Date.now()
      });
      
      // 保持最多3个最近的chunks
      if (this.recentChunks.length > 3) {
        this.recentChunks.shift();
      }
    }
  }
  
  async processSegment(audioSegment, isTimeoutChunk) {
    // 1. 准备prompt
    const prompt = this.previousResults.slice(-2).join(' ');
    
    // 2. 调用Whisper
    const asrResult = await this.asrClient.transcribe(audioSegment, { prompt });
    
    // 3. 可选LLM修正
    let llmResult = null;
    if (this.config.enableLLM) {
      llmResult = await this.llmProcessor.correct(asrResult);
    }
    
    // 4. 更新历史
    this.previousResults.push(asrResult);
    
    // 5. 生成唯一ID
    const segmentId = Date.now();
    
    // 6. 发送结果
    this.sendResult({ 
      segmentId,
      asrResult, 
      llmResult,
      isTimeoutChunk,
      isFinal: !isTimeoutChunk
    });
    
    return segmentId;
  }
  
  async reprocessRecentChunks() {
    // 1. 合并最近的chunks音频
    const combinedAudio = this.combineAudioChunks([
      ...this.recentChunks.map(c => c.audioData),
      this.audioBuffer.extractSegment()  // 当前未满3秒的片段
    ]);
    
    // 2. 获取需要覆盖的segment IDs
    const segmentIdsToReplace = this.recentChunks.map(c => c.id);
    
    // 3. 重新识别合并后的音频
    const prompt = this.previousResults.slice(-4, -this.recentChunks.length).join(' ');
    const asrResult = await this.asrClient.transcribe(combinedAudio, { prompt });
    
    // 4. LLM修正
    let llmResult = null;
    if (this.config.enableLLM) {
      llmResult = await this.llmProcessor.correct(asrResult);
    }
    
    // 5. 发送重识别结果，指示需要覆盖的segments
    this.sendResult({
      segmentId: Date.now(),
      asrResult,
      llmResult,
      isReprocessed: true,
      replacesSegments: segmentIdsToReplace,
      isFinal: true
    });
    
    // 6. 清空recent chunks
    this.recentChunks = [];
    
    // 7. 更新历史记录
    this.previousResults = this.previousResults.slice(0, -segmentIdsToReplace.length);
    this.previousResults.push(asrResult);
  }
}
```

### 3.3 客户端实现

#### 3.3.1 音频采集
```javascript
class AudioCapture {
  async start() {
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
    const audioContext = new AudioContext({ sampleRate: 16000 });
    const source = audioContext.createMediaStreamSource(stream);
    const processor = audioContext.createScriptProcessor(4096, 1, 1);
    
    processor.onaudioprocess = (e) => {
      const audioData = e.inputBuffer.getChannelData(0);
      this.sendAudioData(audioData);
    };
    
    source.connect(processor);
    processor.connect(audioContext.destination);
  }
}
```

#### 3.3.2 WebSocket管理
```javascript
class WebSocketManager {
  constructor() {
    this.segmentDisplay = new Map();  // 存储显示的segments
  }
  
  connect(apiKey) {
    this.ws = new WebSocket('wss://your-server/asr');
    
    this.ws.onopen = () => {
      this.sendConfig({ apiKey });
    };
    
    this.ws.onmessage = (event) => {
      const message = JSON.parse(event.data);
      this.handleServerMessage(message);
    };
    
    // 自动重连逻辑
    this.ws.onclose = () => {
      setTimeout(() => this.reconnect(), 3000);
    };
  }
  
  handleServerMessage(message) {
    if (message.type === 'result') {
      const { segmentId, asrResult, llmResult, isReprocessed, replacesSegments } = message.data;
      
      if (isReprocessed && replacesSegments) {
        // 处理重识别结果：删除旧的segments
        replacesSegments.forEach(id => {
          this.segmentDisplay.delete(id);
        });
      }
      
      // 添加或更新当前segment
      this.segmentDisplay.set(segmentId, {
        asrResult,
        llmResult,
        timestamp: Date.now()
      });
      
      // 更新UI显示
      this.updateDisplay();
    }
  }
  
  updateDisplay() {
    // 按时间顺序合并所有segments的结果
    const allSegments = Array.from(this.segmentDisplay.entries())
      .sort(([idA], [idB]) => idA - idB)
      .map(([_, segment]) => segment);
    
    // 更新ASR显示区域
    const asrText = allSegments.map(s => s.asrResult).join(' ');
    document.getElementById('asr-display').textContent = asrText;
    
    // 更新LLM显示区域（如果启用）
    if (this.config.enableLLM) {
      const llmText = allSegments.map(s => s.llmResult || s.asrResult).join(' ');
      document.getElementById('llm-display').textContent = llmText;
    }
  }
}
```

## 4. 实施计划

### 4.1 开发阶段
1. **第一阶段**（1-2天）
   - 搭建基础WebSocket通信框架
   - 实现音频流传输
   - 集成TEN-VAD

2. **第二阶段**（2-3天）
   - 实现VAD切分逻辑
   - 集成Whisper API
   - 实现基础ASR功能

3. **第三阶段**（1-2天）
   - 添加LLM文本修正
   - 优化prompt策略
   - 完善错误处理

4. **第四阶段**（1天）
   - 性能优化
   - 添加监控指标
   - 编写文档和测试

### 4.2 测试计划
1. **功能测试**
   - VAD准确性测试
   - ASR识别准确率测试
   - LLM修正效果测试

2. **性能测试**
   - 延迟测试（端到端）
   - 并发用户测试
   - 长时间运行稳定性测试

3. **兼容性测试**
   - 不同浏览器测试
   - 不同网络环境测试

## 5. 日志功能设计

### 5.1 日志目录结构
```
vad-based-streaming-asr/
└── log/
    ├── asr/         # ASR请求日志
    ├── vad/         # VAD状态变化日志
    └── wavs/        # 完整录音文件
```

### 5.2 录音文件日志 (log/wavs/)

- **触发时机**: 用户点击"停止录音"按钮时
- **文件格式**: WAV格式，16kHz，单声道，16位
- **命名规则**: `{开始录音时间戳}.wav`
  - 时间戳格式: `YYYYMMDD_HHMMSS_fff`
  - 示例: `20250721_235801_123.wav`
- **内容**: 从点击"开始录音"到"停止录音"的完整音频数据

### 5.3 ASR请求日志 (log/asr/)

#### 5.3.1 音频文件
- **触发时机**: 每次调用Fireworks Whisper API时
- **文件格式**: WAV格式，与API请求一致
- **命名规则**: `{请求时间戳}.wav`
- **内容**: 发送给Whisper API的音频片段

#### 5.3.2 JSON日志文件
- **文件名**: `{请求时间戳}.json`
- **内容格式**:
```json
{
  "timestamp": "20250721_235801_123",
  "request_duration_ms": 1234,
  "text": "识别出的文本内容",
  "text_length": 12,
  "prompt": "前文prompt内容",
  "audio_file": "20250721_235801_123.wav"
}
```

#### 5.3.3 错误日志
- **文件名**: `{请求时间戳}_error.json` 或 `{请求时间戳}_exception.json`
- **内容**: 包含错误信息、状态码、异常详情等

### 5.4 VAD状态日志 (log/vad/)

- **触发时机**: VAD状态从静音→说话或说话→静音时
- **文件格式**: JSON
- **命名规则**: `{状态变化时间戳}.json`
- **内容格式**:
```json
{
  "timestamp": "20250721_235801_123",
  "state": "speech",           // 当前状态
  "previous_state": "silence",  // 之前状态
  "rms": 0.012345,             // 音量RMS值
  "max_amplitude": 0.234567,    // 最大振幅
  "threshold": 0.002           // VAD阈值
}
```

### 5.5 日志管理策略

1. **自动创建目录**: 服务启动时自动创建所需的日志目录结构
2. **异常处理**: 日志写入失败不影响主流程，仅记录错误信息
3. **文件命名**: 使用时间戳确保文件名唯一性，精确到毫秒
4. **存储优化**: 
   - ASR音频文件与请求日志关联存储
   - VAD仅记录状态变化，不保存音频
   - 完整录音仅在停止时保存一次

### 5.6 日志使用场景

1. **调试和分析**:
   - 分析VAD状态变化频率和准确性
   - 检查ASR请求频率和响应时间
   - 验证prompt使用是否正确

2. **性能优化**:
   - 通过request_duration_ms分析API响应时间
   - 通过VAD日志优化静音检测阈值
   - 分析音频片段长度分布

3. **问题排查**:
   - 重放特定时间段的录音
   - 检查ASR识别错误的原因
   - 分析VAD误判情况

### 5.7 日志分析方法

#### 5.7.1 检查日志结构
运行测试脚本验证日志功能：
```bash
python test_logging.py
```

#### 5.7.2 查看ASR请求
```bash
# 列出最新的10个ASR请求
ls -lt log/asr/*.json | head -10

# 查看某个ASR请求的详情
cat log/asr/20250721_235801_123.json | jq .
```

#### 5.7.3 分析VAD状态
```bash
# 统计VAD状态变化次数
ls log/vad/*.json | wc -l

# 查看VAD状态切换模式
for f in log/vad/*.json; do cat $f | jq .state; done
```

#### 5.7.4 播放录音
```bash
# 播放完整录音
play log/wavs/20250721_235801_123.wav

# 播放某个ASR片段  
play log/asr/20250721_235801_456.wav
```

### 5.8 注意事项

1. **存储管理**: 日志文件会持续增长，建议定期清理
2. **磁盘空间**: 录音文件可能较大，注意磁盘空间
3. **性能分析**: 日志包含精确时间戳，可用于性能分析
4. **问题排查**: 出现问题时，请提供相关时间段的完整日志

## 6. 待办事项（TODO）

1. **完成PRD文档编写，包括项目背景、目标、架构设计** ✓
2. **设计WebSocket通信协议和消息格式** ✓
3. **实现服务端WebSocket处理和音频接收** ✓
4. **集成TEN-VAD进行实时语音活动检测** ✓
5. **实现Whisper ASR调用逻辑（含prompt优化）** ✓
6. **设计并实现LLM文本修正功能** ✓
7. **实现网页端UI和实时显示功能** ✓
8. **添加性能监控和时延统计** ✓
9. **实现完整的日志记录功能** ✓
10. **编写测试用例和部署文档**

## 7. 风险与挑战

1. **技术风险**
   - WebSocket连接稳定性
   - VAD准确性对ASR效果的影响
   - API调用频率限制

2. **性能风险**
   - 实时处理的CPU/内存消耗
   - 网络延迟对体验的影响

3. **成本风险**
   - Whisper API调用成本
   - LLM调用成本

## 8. 附录

### 8.1 参考资源
- TEN-VAD文档：[项目内ten-vad目录]
- Whisper API文档：[Fireworks API文档]
- WebSocket最佳实践

### 8.2 配置参数参考
```javascript
const DEFAULT_CONFIG = {
  vad: {
    sampleRate: 16000,
    windowSize: 256,
    threshold: 0.5,
    silenceDuration: 800,  // ms
  },
  asr: {
    model: 'whisper-v3',
    vadModel: 'silero',
    temperature: 0.0,
    responseFormat: 'verbose_json',
  },
  streaming: {
    maxSegmentDuration: 3000,  // ms
    lookbackDuration: 9000,    // ms
    chunkSize: 4096,
  }
};
```

## 9. 核心算法总结（包含Prompt逻辑）

### 9.1 完整的处理流程示例

**场景：用户说话15秒**
```
时间线：[静音1秒][说话15秒][静音]
        ↑       ↑   ↑   ↑   ↑    ↑
        0s      1s  4s  7s  10s  16s

处理过程：
1. 1s：检测到语音开始，记录开始位置
2. 4s：3秒超时，识别[1-4s]
   - Prompt: 之前的历史结果（如果有）
   - 结果："你好"
3. 7s：3秒超时，识别[4-7s]
   - Prompt: "你好"
   - 结果："今天天气"
4. 10s：3秒超时，识别[7-10s]
   - Prompt: "你好 今天天气"
   - 结果："真不错"
5. 13s：3秒超时，识别[10-13s]
   - Prompt: "今天天气 真不错"
   - 结果："我想去"
6. 16s：语音结束
   - 总时长15秒 > 9秒
   - 回看9秒：16-9=7s
   - 找到7s位置对应的chunk边界（正好是7s）
   - 重识别[7-16s]，共9秒
   - Prompt: "你好 今天天气"（7s之前的最后2个）
   - 新结果："真不错我想去公园走走"
   - 删除原来的[7-10s]"真不错"和[10-13s]"我想去"
   - 最终显示："你好" "今天天气" "真不错我想去公园走走"
```

### 9.2 Prompt选择的关键原则

1. **不使用即将被覆盖的内容**：避免循环依赖
2. **使用稳定的历史结果**：选择不会被修改的部分
3. **保持上下文连贯**：尽可能使用相邻的前文
4. **限制prompt长度**：最多使用2个历史结果，避免过长影响性能