# ç§»åŠ¨ç«¯ä¸“ç”¨ API ä½¿ç”¨æŒ‡å— (Expo React Native)

## API æ¦‚è¿°

æœ¬æŒ‡å—ä¸“é—¨é’ˆå¯¹ **Expo React Native** åº”ç”¨å¼€å‘ï¼Œä½¿ç”¨ **@siteed/expo-audio-studio** è¿›è¡ŒéŸ³é¢‘å½•åˆ¶ï¼Œå¹¶ä¸ ASR API æœåŠ¡é›†æˆè¿›è¡Œè¯­éŸ³æ´»åŠ¨æ£€æµ‹(VAD)å’ŒéŸ³é¢‘å¤„ç†ã€‚

### æ ¸å¿ƒç‰¹æ€§

1. **é«˜è´¨é‡éŸ³é¢‘å½•åˆ¶** - ä½¿ç”¨ expo-audio-stream è¿›è¡Œä¸“ä¸šçº§å½•éŸ³
2. **å®æ—¶ VAD æ£€æµ‹** - é›†æˆ TEN-VAD è¿›è¡Œç²¾ç¡®è¯­éŸ³æ£€æµ‹  
3. **å¤šç§ä¼ è¾“æ–¹å¼** - REST APIã€WebSocketæµå¼å¤„ç†
4. **æ ¼å¼è‡ªåŠ¨è½¬æ¢** - æ”¯æŒå¤šç§éŸ³é¢‘æ ¼å¼è½¬æ¢
5. **æ€§èƒ½ä¼˜åŒ–** - é¿å…Base64è†¨èƒ€ï¼Œæå‡ä¼ è¾“æ•ˆç‡

## ä¾èµ–å®‰è£…

```bash
# å®‰è£…å¿…éœ€çš„åŒ…
npm install @siteed/expo-audio-studio expo-audio expo-file-system

# å¦‚æœä½¿ç”¨WebSocket
npm install ws
```

## API ç«¯ç‚¹æ¦‚è§ˆ

æœ¬æ–‡æ¡£æ¶µç›–ä¸‰ç§ä¸»è¦çš„éŸ³é¢‘å¤„ç†æ–¹å¼ï¼š
1. **HTTP REST API** - é€‚åˆå½•éŸ³å®Œæˆåçš„æ‰¹é‡å¤„ç†
2. **WebSocketæµå¼API** - é€‚åˆå®æ—¶éŸ³é¢‘æµå¤„ç†
3. **ä¼˜åŒ–æ–‡ä»¶ä¸Šä¼ API** - é¿å…Base64ç¼–ç ï¼Œæ€§èƒ½æœ€ä½³

### 1. ä¸»è¦å¤„ç†ç«¯ç‚¹ï¼š`/api/v1/mobile/process-audio`

è¿™æ˜¯æœ€ä¸»è¦çš„ç«¯ç‚¹ï¼Œæä¾›å®Œæ•´çš„éŸ³é¢‘å¤„ç†åŠŸèƒ½ã€‚

#### è¯·æ±‚æ ¼å¼

```typescript
interface MobileAudioRequest {
  // å¿…éœ€å­—æ®µ
  audio_base64: string;    // Base64 ç¼–ç çš„éŸ³é¢‘æ•°æ®
  format: string;          // éŸ³é¢‘æ ¼å¼: "wav", "m4a", "mp3", "webm" ç­‰
  sample_rate: number;     // é‡‡æ ·ç‡ï¼Œé»˜è®¤ 16000
  
  // VAD å‚æ•°
  enable_vad: boolean;           // æ˜¯å¦å¯ç”¨ VADï¼Œé»˜è®¤ true
  vad_window_duration: number;   // VAD çª—å£æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ 0.5
  vad_overlap: number;           // VAD çª—å£é‡å ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ 0.1
  
  // è¿”å›é€‰é¡¹
  return_format: string;    // "segments" | "base64" | "merged"
  compress_output: boolean; // æ˜¯å¦å‹ç¼©è¾“å‡ºï¼Œé»˜è®¤ false
}
```

#### å“åº”æ ¼å¼

```typescript
interface MobileAudioResponse {
  success: boolean;
  message: string;
  
  // VAD ç»“æœ
  has_speech: boolean;              // æ˜¯å¦æ£€æµ‹åˆ°è¯­éŸ³
  speech_segments: Array<{          // è¯­éŸ³æ®µåˆ—è¡¨
    start: number;
    end: number;
    duration: number;
  }>;
  speech_ratio: number;             // è¯­éŸ³å æ¯” (0-1)
  total_duration: number;           // æ€»æ—¶é•¿ï¼ˆç§’ï¼‰
  
  // éŸ³é¢‘æ•°æ®ï¼ˆæ ¹æ® return_formatï¼‰
  audio_data?: number[];            // PCM æµ®ç‚¹æ•°ç»„
  audio_base64?: string;            // Base64 ç¼–ç çš„éŸ³é¢‘
  audio_format?: string;            // éŸ³é¢‘æ ¼å¼
  
  // æ€§èƒ½æŒ‡æ ‡
  processing_time_ms: number;       // å¤„ç†æ—¶é—´
  audio_size_bytes?: number;        // éŸ³é¢‘å¤§å°
}
```

#### ä½¿ç”¨ç¤ºä¾‹

```javascript
// Expo React Native ä¸­çš„ä½¿ç”¨
import * as FileSystem from 'expo-file-system';
import axios from 'axios';

async function processRecording(audioUri) {
  // è¯»å–å½•éŸ³æ–‡ä»¶
  const audioBase64 = await FileSystem.readAsStringAsync(audioUri, {
    encoding: FileSystem.EncodingType.Base64,
  });
  
  // å‘é€åˆ° API
  const response = await axios.post('http://your-api.com/api/v1/mobile/process-audio', {
    audio_base64: audioBase64,
    format: 'm4a',  // Expo é»˜è®¤å½•åˆ¶æ ¼å¼
    sample_rate: 16000,
    enable_vad: true,
    return_format: 'base64',  // è¿”å›å¤„ç†åçš„éŸ³é¢‘
  });
  
  if (response.data.success && response.data.has_speech) {
    console.log('æ£€æµ‹åˆ°è¯­éŸ³ï¼Œæ—¶é•¿:', response.data.total_duration);
    console.log('è¯­éŸ³æ®µ:', response.data.speech_segments);
    
    // ä½¿ç”¨å¤„ç†åçš„éŸ³é¢‘
    const processedAudio = response.data.audio_base64;
    // å‘é€åˆ°åç«¯æˆ–æ’­æ”¾
  }
}
```

### 2. å¿«é€Ÿæ£€æµ‹ç«¯ç‚¹ï¼š`/api/v1/mobile/quick-vad`

ç”¨äºå¿«é€Ÿæ£€æµ‹éŸ³é¢‘æ˜¯å¦åŒ…å«è¯­éŸ³ï¼Œå“åº”æå¿«ã€‚

#### è¯·æ±‚ï¼ˆForm Dataï¼‰

```javascript
const formData = new FormData();
formData.append('audio_base64', audioBase64String);
formData.append('format', 'wav');
formData.append('sample_rate', '16000');

const response = await fetch('/api/v1/mobile/quick-vad', {
  method: 'POST',
  body: formData,
});
```

#### å“åº”

```json
{
  "has_speech": true,
  "rms": 0.125,
  "duration": 3.5,
  "processing_time_ms": 45
}
```

### 3. æ‰¹é‡å¤„ç†ç«¯ç‚¹ï¼š`/api/v1/mobile/batch-process`

å¤„ç†å¤šä¸ªéŸ³é¢‘æ–‡ä»¶ï¼Œå¯é€‰æ‹©åˆå¹¶ç»“æœã€‚

#### ä½¿ç”¨ç¤ºä¾‹

```javascript
const formData = new FormData();

// æ·»åŠ å¤šä¸ªéŸ³é¢‘æ–‡ä»¶
audioFiles.forEach((file, index) => {
  formData.append('audio_files', file);
});

formData.append('enable_vad', 'true');
formData.append('merge_results', 'true');  // åˆå¹¶æ‰€æœ‰æœ‰æ•ˆéŸ³é¢‘

const response = await fetch('/api/v1/mobile/batch-process', {
  method: 'POST',
  body: formData,
});

const result = await response.json();
// result.merged_audio åŒ…å«åˆå¹¶åçš„éŸ³é¢‘
```

### 4. é«˜æ•ˆå¤„ç†ç«¯ç‚¹ï¼š`/api/v1/mobile/process-audio-efficient` âš¡

**æ¨èä½¿ç”¨**ï¼šé¿å…Base64ç¼–ç ï¼Œç›´æ¥æ–‡ä»¶ä¸Šä¼ ï¼Œæ€§èƒ½æå‡33%ã€‚

#### è¯·æ±‚æ ¼å¼

```javascript
// ä½¿ç”¨ FormData ç›´æ¥ä¸Šä¼ æ–‡ä»¶
const formData = new FormData();
formData.append('audio', {
  uri: audioUri,        // éŸ³é¢‘æ–‡ä»¶è·¯å¾„
  type: 'audio/wav',    // MIMEç±»å‹
  name: 'recording.wav', // æ–‡ä»¶å
});
formData.append('sample_rate', '16000');
formData.append('enable_vad', 'true');
formData.append('return_audio', 'false');  // æ˜¯å¦è¿”å›å¤„ç†åçš„éŸ³é¢‘
formData.append('output_format', 'json');  // 'json' æˆ– 'binary'

const response = await fetch('/api/v1/mobile/process-audio-efficient', {
  method: 'POST',
  body: formData,
  headers: {
    'Content-Type': 'multipart/form-data',
  },
});
```

#### ä¼˜åŠ¿

- âœ… **æ€§èƒ½æå‡33%** - æ— Base64ç¼–ç å¼€é”€
- âœ… **å†…å­˜ä½¿ç”¨å‡åŠ** - é¿å…æ•°æ®é‡å¤å­˜å‚¨
- âœ… **æ”¯æŒæ‰€æœ‰æ ¼å¼** - WAV, M4A, MP3, OGGç­‰
- âœ… **ç§»åŠ¨ç«¯å‹å¥½** - ç›´æ¥ä½¿ç”¨å½•éŸ³æ–‡ä»¶

### 5. WebSocketæµå¼VADå¤„ç† ğŸš€

é€‚ç”¨äº**å®æ—¶éŸ³é¢‘å¤„ç†**ï¼Œæ€§èƒ½æå‡228%ã€‚

#### è¿æ¥åœ°å€

```javascript
// JSONæ ¼å¼æµï¼ˆè°ƒè¯•å‹å¥½ï¼‰
const wsUrl = 'ws://your-api.com/api/v1/stream/vad';

// äºŒè¿›åˆ¶æ ¼å¼æµï¼ˆæœ€é«˜æ€§èƒ½ï¼‰
const wsUrl = 'ws://your-api.com/api/v1/stream/vad-binary';
```

#### WebSocket JSONæµä½¿ç”¨ç¤ºä¾‹

```javascript
import WebSocket from 'ws';

class StreamVADProcessor {
  constructor(url) {
    this.ws = new WebSocket(url);
    this.setupEventHandlers();
  }

  setupEventHandlers() {
    this.ws.onopen = () => {
      console.log('WebSocketè¿æ¥å»ºç«‹');
      
      // å‘é€é…ç½®
      this.ws.send(JSON.stringify({
        type: 'config',
        sample_rate: 16000,
        channels: 1
      }));
    };

    this.ws.onmessage = (event) => {
      const message = JSON.parse(event.data);
      
      switch (message.type) {
        case 'status':
          console.log('çŠ¶æ€:', message.message);
          console.log('ä½¿ç”¨TEN-VAD:', message.use_real_vad);
          break;
          
        case 'vad':
          console.log('VADç»“æœ:', {
            is_speaking: message.is_speaking,
            probability: message.probability,
            current_state: message.current_state,
            processing_time: message.processing_time_ms
          });
          
          // æ ¹æ®VADç»“æœè¿›è¡Œå¤„ç†
          if (message.is_speaking) {
            this.onSpeechDetected(message);
          } else {
            this.onSilenceDetected(message);
          }
          break;
          
        case 'error':
          console.error('å¤„ç†é”™è¯¯:', message.message);
          break;
      }
    };

    this.ws.onerror = (error) => {
      console.error('WebSocketé”™è¯¯:', error);
    };

    this.ws.onclose = () => {
      console.log('WebSocketè¿æ¥å…³é—­');
    };
  }

  // å‘é€éŸ³é¢‘æ•°æ®
  sendAudioChunk(audioFloatArray) {
    if (this.ws.readyState === WebSocket.OPEN) {
      this.ws.send(JSON.stringify({
        type: 'audio',
        data: Array.from(audioFloatArray)
      }));
    }
  }

  // ç»“æŸå¤„ç†
  endProcessing() {
    if (this.ws.readyState === WebSocket.OPEN) {
      this.ws.send(JSON.stringify({
        type: 'end'
      }));
    }
  }

  onSpeechDetected(vadResult) {
    // æ£€æµ‹åˆ°è¯­éŸ³æ—¶çš„å¤„ç†é€»è¾‘
    console.log(`æ£€æµ‹åˆ°è¯­éŸ³ (ç½®ä¿¡åº¦: ${vadResult.probability})`);
  }

  onSilenceDetected(vadResult) {
    // æ£€æµ‹åˆ°é™éŸ³æ—¶çš„å¤„ç†é€»è¾‘
    if (vadResult.silence_timeout) {
      console.log('é™éŸ³è¶…æ—¶ï¼Œå¯èƒ½è¯­éŸ³ç»“æŸ');
    }
  }

  close() {
    this.ws.close();
  }
}

// ä½¿ç”¨ç¤ºä¾‹
const vadProcessor = new StreamVADProcessor('ws://localhost:8000/api/v1/stream/vad');

// æ¨¡æ‹Ÿå®æ—¶éŸ³é¢‘æµ
const simulateAudioStream = () => {
  const sampleRate = 16000;
  const chunkSize = 1024;
  
  setInterval(() => {
    // ç”Ÿæˆæ¨¡æ‹ŸéŸ³é¢‘æ•°æ®ï¼ˆå®é™…åº”ç”¨ä¸­ä»éº¦å…‹é£è·å–ï¼‰
    const audioChunk = new Float32Array(chunkSize);
    for (let i = 0; i < chunkSize; i++) {
      audioChunk[i] = (Math.random() - 0.5) * 0.1; // éšæœºå™ªéŸ³
    }
    
    vadProcessor.sendAudioChunk(audioChunk);
  }, (chunkSize / sampleRate) * 1000); // å®æ—¶é€Ÿåº¦
};

// å¼€å§‹æ¨¡æ‹Ÿ
simulateAudioStream();
```

#### WebSocketäºŒè¿›åˆ¶æµä½¿ç”¨ç¤ºä¾‹ï¼ˆæœ€é«˜æ€§èƒ½ï¼‰

```javascript
class BinaryStreamVADProcessor {
  constructor(url) {
    this.ws = new WebSocket(url);
    this.configured = false;
    this.setupEventHandlers();
  }

  setupEventHandlers() {
    this.ws.onopen = () => {
      // å‘é€é…ç½®ï¼ˆæ–‡æœ¬æ ¼å¼ï¼‰
      this.ws.send(JSON.stringify({
        sample_rate: 16000,
        window_size: 1024
      }));
    };

    this.ws.onmessage = (event) => {
      if (typeof event.data === 'string') {
        // é…ç½®å“åº”æˆ–VADç»“æœ
        const message = JSON.parse(event.data);
        
        if (message.type === 'ready') {
          console.log('WebSocketé…ç½®å®Œæˆ:', message);
          this.configured = true;
        } else {
          // VADå¤„ç†ç»“æœ
          console.log('VADç»“æœ:', {
            is_speaking: message.is_speaking,
            probability: message.probability,
            processing_time: message.processing_time_ms,
            samples: message.samples
          });
        }
      }
    };
  }

  // å‘é€äºŒè¿›åˆ¶éŸ³é¢‘æ•°æ®
  sendAudioBinary(audioFloatArray) {
    if (this.configured && this.ws.readyState === WebSocket.OPEN) {
      // ç›´æ¥å‘é€Float32æ•°ç»„çš„äºŒè¿›åˆ¶æ•°æ®
      const buffer = audioFloatArray.buffer;
      this.ws.send(buffer);
    }
  }

  // ç»“æŸå¤„ç†
  endProcessing() {
    if (this.ws.readyState === WebSocket.OPEN) {
      this.ws.send(new ArrayBuffer(0)); // ç©ºbufferè¡¨ç¤ºç»“æŸ
    }
  }
}

// ä½¿ç”¨ç¤ºä¾‹
const binaryProcessor = new BinaryStreamVADProcessor('ws://localhost:8000/api/v1/stream/vad-binary');

// å®æ—¶éŸ³é¢‘å¤„ç†
const processRealTimeAudio = (audioBuffer) => {
  const float32Array = new Float32Array(audioBuffer);
  binaryProcessor.sendAudioBinary(float32Array);
};
```

### å®æ—¶WebSocketæµå¼VADç»„ä»¶

```typescript
// StreamingVADComponent.tsx
import React, { useEffect, useState, useRef } from 'react';
import { View, Text, TouchableOpacity, StyleSheet } from 'react-native';
import { 
  useAudioRecorder, 
  RecordingConfig,
  ExpoAudioStreamModule 
} from '@siteed/expo-audio-studio';

interface VADResult {
  is_speaking: boolean;
  probability: number;
  current_state: string;
  processing_time_ms: number;
}

const API_WS_URL = 'ws://your-api.com';

export function StreamingVADComponent() {
  const [isConnected, setIsConnected] = useState(false);
  const [vadResult, setVadResult] = useState<VADResult | null>(null);
  const [connectionError, setConnectionError] = useState<string | null>(null);
  const wsRef = useRef<WebSocket | null>(null);
  const audioBufferRef = useRef<Float32Array[]>([]);
  
  // ä½¿ç”¨ expo-audio-studio è¿›è¡Œå®æ—¶å½•éŸ³
  const {
    startRecording,
    stopRecording,
    isRecording,
    durationMs,
  } = useAudioRecorder();

  const connectWebSocket = () => {
    try {
      const ws = new WebSocket(`${API_WS_URL}/api/v1/stream/vad`);
      
      ws.onopen = () => {
        console.log('WebSocketè¿æ¥æˆåŠŸ');
        setIsConnected(true);
        setConnectionError(null);
        
        // å‘é€é…ç½®
        ws.send(JSON.stringify({
          type: 'config',
          sample_rate: 16000,
          channels: 1
        }));
      };

      ws.onmessage = (event) => {
        try {
          const message = JSON.parse(event.data);
          
          switch (message.type) {
            case 'status':
              console.log('WebSocketçŠ¶æ€:', message.message);
              console.log('ä½¿ç”¨TEN-VAD:', message.use_real_vad);
              break;
              
            case 'vad':
              setVadResult({
                is_speaking: message.is_speaking,
                probability: message.probability,
                current_state: message.current_state,
                processing_time_ms: message.processing_time_ms
              });
              
              // å¯é€‰ï¼šåŸºäºVADç»“æœè¿›è¡Œå…¶ä»–æ“ä½œ
              if (message.is_speaking) {
                onSpeechDetected(message);
              } else if (message.silence_timeout) {
                onSilenceTimeout();
              }
              break;
              
            case 'error':
              console.error('WebSocketå¤„ç†é”™è¯¯:', message.message);
              setConnectionError(message.message);
              break;
          }
        } catch (error) {
          console.error('è§£æWebSocketæ¶ˆæ¯å¤±è´¥:', error);
        }
      };

      ws.onerror = (error) => {
        console.error('WebSocketè¿æ¥é”™è¯¯:', error);
        setConnectionError('è¿æ¥å¤±è´¥');
      };

      ws.onclose = (event) => {
        console.log('WebSocketè¿æ¥å…³é—­:', event.code, event.reason);
        setIsConnected(false);
        
        // è‡ªåŠ¨é‡è¿ï¼ˆå¯é€‰ï¼‰
        if (!event.wasClean && event.code !== 1000) {
          setTimeout(() => {
            console.log('å°è¯•é‡æ–°è¿æ¥WebSocket...');
            connectWebSocket();
          }, 3000);
        }
      };

      wsRef.current = ws;
    } catch (error) {
      console.error('åˆ›å»ºWebSocketè¿æ¥å¤±è´¥:', error);
      setConnectionError('æ— æ³•åˆ›å»ºè¿æ¥');
    }
  };

  const startStreamingRecording = async () => {
    try {
      // ç¡®ä¿WebSocketå·²è¿æ¥
      if (!isConnected) {
        setConnectionError('WebSocketæœªè¿æ¥');
        return;
      }

      // è¯·æ±‚æƒé™
      const { status } = await ExpoAudioStreamModule.requestPermissionsAsync();
      if (status !== 'granted') {
        setConnectionError('éœ€è¦éº¦å…‹é£æƒé™');
        return;
      }

      // æ¸…ç©ºéŸ³é¢‘ç¼“å†²åŒº
      audioBufferRef.current = [];

      // é…ç½®å®æ—¶å½•éŸ³
      const config: RecordingConfig = {
        interval: 100,          // æ›´é¢‘ç¹çš„å›è°ƒï¼Œç”¨äºå®æ—¶æµ
        enableProcessing: true,
        sampleRate: 16000,
        channels: 1,
        encoding: 'pcm_16bit',
        
        // å…³é”®ï¼šå®æ—¶éŸ³é¢‘æµå›è°ƒ
        onAudioStream: async (audioData) => {
          await sendAudioToWebSocket(audioData);
        },
        
        compression: {
          enabled: false,  // ä¸å‹ç¼©ï¼Œä¿æŒå®æ—¶æ€§
        },
        
        keepAwake: true,
        showNotification: false,  // æµå¼å¤„ç†ä¸æ˜¾ç¤ºé€šçŸ¥
      };

      await startRecording(config);
      console.log('å¼€å§‹æµå¼å½•éŸ³');
      
    } catch (error) {
      console.error('å¼€å§‹æµå¼å½•éŸ³å¤±è´¥:', error);
      setConnectionError('å½•éŸ³å¯åŠ¨å¤±è´¥');
    }
  };

  const sendAudioToWebSocket = async (audioData: any) => {
    if (!wsRef.current || wsRef.current.readyState !== WebSocket.OPEN) {
      return;
    }

    try {
      // audioData.buffer åŒ…å«Float32Arrayæ ¼å¼çš„éŸ³é¢‘æ•°æ®
      const audioArray = Array.from(audioData.buffer);
      
      // å‘é€éŸ³é¢‘æ•°æ®åˆ°WebSocket
      wsRef.current.send(JSON.stringify({
        type: 'audio',
        data: audioArray
      }));
      
      // å¯é€‰ï¼šæœ¬åœ°ç¼“å­˜éŸ³é¢‘æ•°æ®
      audioBufferRef.current.push(audioData.buffer);
      
      // é™åˆ¶ç¼“å†²åŒºå¤§å°ï¼Œé¿å…å†…å­˜æº¢å‡º
      if (audioBufferRef.current.length > 100) {
        audioBufferRef.current.shift();
      }
      
    } catch (error) {
      console.error('å‘é€éŸ³é¢‘æ•°æ®å¤±è´¥:', error);
    }
  };

  const stopStreamingRecording = async () => {
    try {
      await stopRecording();
      
      // å‘é€ç»“æŸä¿¡å·
      if (wsRef.current && wsRef.current.readyState === WebSocket.OPEN) {
        wsRef.current.send(JSON.stringify({ type: 'end' }));
      }
      
      console.log('åœæ­¢æµå¼å½•éŸ³');
      
    } catch (error) {
      console.error('åœæ­¢æµå¼å½•éŸ³å¤±è´¥:', error);
    }
  };

  const onSpeechDetected = (vadResult: any) => {
    console.log(`æ£€æµ‹åˆ°è¯­éŸ³ (ç½®ä¿¡åº¦: ${vadResult.probability.toFixed(3)})`);
    // åœ¨è¿™é‡Œå¯ä»¥æ·»åŠ è¯­éŸ³æ£€æµ‹åçš„é€»è¾‘
    // ä¾‹å¦‚ï¼šå¼€å§‹è¯­éŸ³è¯†åˆ«ã€è§¦å‘UIåé¦ˆç­‰
  };

  const onSilenceTimeout = () => {
    console.log('é™éŸ³è¶…æ—¶ï¼Œå¯èƒ½è¯­éŸ³ç»“æŸ');
    // åœ¨è¿™é‡Œå¯ä»¥æ·»åŠ é™éŸ³è¶…æ—¶åçš„é€»è¾‘
    // ä¾‹å¦‚ï¼šç»“æŸè¯­éŸ³è¯†åˆ«ã€ä¿å­˜å½“å‰æ®µè½ç­‰
  };

  const disconnectWebSocket = () => {
    if (wsRef.current) {
      wsRef.current.close(1000, 'ç”¨æˆ·æ‰‹åŠ¨æ–­å¼€');
      wsRef.current = null;
    }
  };

  useEffect(() => {
    connectWebSocket();
    
    return () => {
      disconnectWebSocket();
    };
  }, []);

  return (
    <View style={styles.container}>
      <Text style={styles.title}>å®æ—¶VADæµå¼å¤„ç†</Text>
      
      {/* WebSocketè¿æ¥çŠ¶æ€ */}
      <View style={styles.statusContainer}>
        <Text style={styles.statusText}>
          WebSocket: {isConnected ? 'âœ… å·²è¿æ¥' : 'âŒ æœªè¿æ¥'}
        </Text>
        {connectionError && (
          <Text style={[styles.statusText, styles.errorText]}>
            é”™è¯¯: {connectionError}
          </Text>
        )}
      </View>
      
      {/* VADç»“æœæ˜¾ç¤º */}
      {vadResult && isConnected && (
        <View style={styles.vadContainer}>
          <Text style={styles.vadTitle}>å®æ—¶VADæ£€æµ‹ç»“æœ</Text>
          <Text style={styles.vadText}>
            çŠ¶æ€: <Text style={styles.vadValue}>{vadResult.current_state}</Text>
          </Text>
          <Text style={styles.vadText}>
            æ˜¯å¦è¯´è¯: <Text style={styles.vadValue}>
              {vadResult.is_speaking ? 'ğŸ—£ï¸ æ˜¯' : 'ğŸ¤« å¦'}
            </Text>
          </Text>
          <Text style={styles.vadText}>
            ç½®ä¿¡åº¦: <Text style={styles.vadValue}>
              {vadResult.probability.toFixed(3)}
            </Text>
          </Text>
          <Text style={styles.vadText}>
            å¤„ç†æ—¶é—´: <Text style={styles.vadValue}>
              {vadResult.processing_time_ms}ms
            </Text>
          </Text>
        </View>
      )}

      {/* å½•éŸ³çŠ¶æ€æ˜¾ç¤º */}
      {isRecording && (
        <View style={styles.recordingContainer}>
          <Text style={styles.recordingText}>
            ğŸ”´ æ­£åœ¨å½•éŸ³: {(durationMs / 1000).toFixed(1)}ç§’
          </Text>
          <Text style={styles.recordingText}>
            ç¼“å†²åŒº: {audioBufferRef.current.length} å—
          </Text>
        </View>
      )}

      {/* æ§åˆ¶æŒ‰é’® */}
      <View style={styles.buttonContainer}>
        {!isConnected && (
          <TouchableOpacity
            style={[styles.button, styles.connectButton]}
            onPress={connectWebSocket}
          >
            <Text style={styles.buttonText}>é‡æ–°è¿æ¥</Text>
          </TouchableOpacity>
        )}
        
        <TouchableOpacity
          style={[
            styles.button,
            isRecording ? styles.stopButton : styles.startButton
          ]}
          onPress={isRecording ? stopStreamingRecording : startStreamingRecording}
          disabled={!isConnected}
        >
          <Text style={styles.buttonText}>
            {isRecording ? 'åœæ­¢æµå¼å½•éŸ³' : 'å¼€å§‹æµå¼å½•éŸ³'}
          </Text>
        </TouchableOpacity>
      </View>
    </View>
  );
}

const styles = StyleSheet.create({
  container: {
    flex: 1,
    padding: 20,
    backgroundColor: '#f5f5f5',
  },
  title: {
    fontSize: 24,
    fontWeight: 'bold',
    textAlign: 'center',
    marginBottom: 20,
    color: '#333',
  },
  statusContainer: {
    backgroundColor: '#fff',
    padding: 15,
    borderRadius: 10,
    marginBottom: 15,
    shadowColor: '#000',
    shadowOffset: { width: 0, height: 1 },
    shadowOpacity: 0.1,
    shadowRadius: 2,
    elevation: 2,
  },
  statusText: {
    fontSize: 16,
    color: '#666',
    marginBottom: 5,
  },
  errorText: {
    color: '#FF3B30',
    fontWeight: 'bold',
  },
  vadContainer: {
    backgroundColor: '#fff',
    padding: 15,
    borderRadius: 10,
    marginBottom: 15,
    borderLeftWidth: 4,
    borderLeftColor: '#007AFF',
    shadowColor: '#000',
    shadowOffset: { width: 0, height: 1 },
    shadowOpacity: 0.1,
    shadowRadius: 2,
    elevation: 2,
  },
  vadTitle: {
    fontSize: 18,
    fontWeight: 'bold',
    color: '#333',
    marginBottom: 10,
  },
  vadText: {
    fontSize: 16,
    color: '#666',
    marginBottom: 5,
  },
  vadValue: {
    fontWeight: 'bold',
    color: '#333',
  },
  recordingContainer: {
    backgroundColor: '#FFE5E5',
    padding: 15,
    borderRadius: 10,
    marginBottom: 15,
    borderLeftWidth: 4,
    borderLeftColor: '#FF3B30',
  },
  recordingText: {
    fontSize: 16,
    color: '#FF3B30',
    fontWeight: 'bold',
    marginBottom: 5,
  },
  buttonContainer: {
    flexDirection: 'column',
    gap: 10,
  },
  button: {
    padding: 15,
    borderRadius: 10,
    alignItems: 'center',
    shadowColor: '#000',
    shadowOffset: { width: 0, height: 2 },
    shadowOpacity: 0.1,
    shadowRadius: 4,
    elevation: 3,
  },
  connectButton: {
    backgroundColor: '#FF9500',
  },
  startButton: {
    backgroundColor: '#34C759',
  },
  stopButton: {
    backgroundColor: '#FF3B30',
  },
  buttonText: {
    color: 'white',
    fontSize: 16,
    fontWeight: 'bold',
  },
});
```

### 6. æ€§èƒ½å¯¹æ¯”æ€»ç»“

| æ–¹æ¡ˆ | å»¶è¿Ÿ | æ•°æ®é‡ | æ€§èƒ½æå‡ | é€‚ç”¨åœºæ™¯ |
|------|------|--------|----------|----------|
| **Base64 REST API** | é«˜ | 133% | åŸºå‡† | å…¼å®¹æ€§è¦æ±‚é«˜ |
| **é«˜æ•ˆæ–‡ä»¶ä¸Šä¼ ** | ä¸­ | 100% | **33%æ›´å¿«** âš¡ | ç§»åŠ¨APPæ¨è |
| **WebSocket JSON** | ä½ | 100% | **130%æ›´å¿«** âš¡ | å®æ—¶+è°ƒè¯• |
| **WebSocketäºŒè¿›åˆ¶** | æœ€ä½ | 100% | **228%æ›´å¿«** ğŸš€ | å®æ—¶éŸ³é¢‘æµ |

## å®Œæ•´çš„ Expo React Native é›†æˆç¤ºä¾‹

### åŸºç¡€å½•éŸ³ç»„ä»¶

```typescript
// VoiceRecorderWithAPI.tsx
import React, { useState } from 'react';
import { View, TouchableOpacity, Text, Alert, StyleSheet } from 'react-native';
import { 
  useAudioRecorder, 
  RecordingConfig,
  AudioRecording,
  ExpoAudioStreamModule 
} from '@siteed/expo-audio-studio';
import { useAudioPlayer } from 'expo-audio';
import * as FileSystem from 'expo-file-system';

const API_BASE_URL = 'http://your-api.com';

export function VoiceRecorderWithAPI() {
  const [isProcessing, setIsProcessing] = useState(false);
  const [audioResult, setAudioResult] = useState<AudioRecording | null>(null);
  
  // ä½¿ç”¨ expo-audio-studio çš„å½•éŸ³ hook
  const {
    startRecording,
    stopRecording,
    pauseRecording,
    resumeRecording,
    durationMs,
    size,
    isRecording,
    isPaused,
  } = useAudioRecorder();

  // ä½¿ç”¨ expo-audio çš„æ’­æ”¾å™¨
  const player = useAudioPlayer(audioResult?.fileUri ?? "");

  const handleStartRecording = async () => {
    try {
      // è¯·æ±‚æƒé™
      const { status } = await ExpoAudioStreamModule.requestPermissionsAsync();
      if (status !== 'granted') {
        Alert.alert('æƒé™é”™è¯¯', 'éœ€è¦éº¦å…‹é£æƒé™æ‰èƒ½å½•éŸ³');
        return;
      }

      // é…ç½®å½•éŸ³å‚æ•° - ä¼˜åŒ–ç”¨äºè¯­éŸ³è¯†åˆ«
      const config: RecordingConfig = {
        interval: 500,           // æ¯500msæŠ¥å‘Šä¸€æ¬¡çŠ¶æ€
        enableProcessing: true,  // å¯ç”¨éŸ³é¢‘å¤„ç†
        sampleRate: 16000,      // 16kHzé‡‡æ ·ç‡ï¼Œé€‚åˆè¯­éŸ³è¯†åˆ«
        channels: 1,            // å•å£°é“
        encoding: 'pcm_16bit',  // 16ä½PCMç¼–ç 
        
        // éŸ³é¢‘å‹ç¼©é…ç½®
        compression: {
          enabled: false,       // ä¸å‹ç¼©ï¼Œä¿æŒåŸå§‹è´¨é‡
          format: 'aac',
          bitrate: 128000,
        },
        
        // å½•éŸ³è¢«ä¸­æ–­æ—¶è‡ªåŠ¨æ¢å¤
        autoResumeAfterInterruption: true,
        
        // ä¿æŒè®¾å¤‡å”¤é†’
        keepAwake: true,
        
        // æ˜¾ç¤ºå½•éŸ³é€šçŸ¥
        showNotification: true,
        
        // å¯é€‰ï¼šå®æ—¶éŸ³é¢‘æµå¤„ç†
        onAudioStream: async (audioData) => {
          console.log('å®æ—¶éŸ³é¢‘æ•°æ®:', audioData.buffer.length);
        },
        
        // å¯é€‰ï¼šå½•éŸ³ä¸­æ–­å¤„ç†
        onRecordingInterrupted: (event) => {
          console.log('å½•éŸ³è¢«ä¸­æ–­:', event.reason);
        },
      };

      await startRecording(config);
    } catch (error) {
      console.error('å¼€å§‹å½•éŸ³å¤±è´¥:', error);
      Alert.alert('é”™è¯¯', 'æ— æ³•å¼€å§‹å½•éŸ³');
    }
  };

  const handleStopRecording = async () => {
    try {
      setIsProcessing(true);
      
      // åœæ­¢å½•éŸ³å¹¶è·å–ç»“æœ
      const result = await stopRecording();
      setAudioResult(result);
      
      if (!result.fileUri) {
        throw new Error('å½•éŸ³æ–‡ä»¶åˆ›å»ºå¤±è´¥');
      }

      console.log('å½•éŸ³å®Œæˆ:', {
        fileUri: result.fileUri,
        duration: durationMs / 1000,
        size: size,
      });

      // å¤„ç†å½•éŸ³æ–‡ä»¶
      await processAudioFile(result.fileUri);

    } catch (error) {
      console.error('åœæ­¢å½•éŸ³å¤±è´¥:', error);
      Alert.alert('é”™è¯¯', 'åœæ­¢å½•éŸ³å¤±è´¥');
    } finally {
      setIsProcessing(false);
    }
  };

  const processAudioFile = async (fileUri: string) => {
    try {
      // æ–¹æ³•1: ä½¿ç”¨ä¼˜åŒ–çš„æ–‡ä»¶ä¸Šä¼ APIï¼ˆæ¨èï¼‰
      await processWithFileUpload(fileUri);
      
      // æ–¹æ³•2: ä½¿ç”¨Base64ç¼–ç ï¼ˆå¤‡ç”¨ï¼‰
      // await processWithBase64(fileUri);
      
    } catch (error) {
      console.error('éŸ³é¢‘å¤„ç†å¤±è´¥:', error);
      Alert.alert('å¤„ç†å¤±è´¥', error.message);
    }
  };

  // æ¨èæ–¹æ³•ï¼šç›´æ¥æ–‡ä»¶ä¸Šä¼ ï¼Œé¿å…Base64ç¼–ç 
  const processWithFileUpload = async (fileUri: string) => {
    const formData = new FormData();
    
    // å‡†å¤‡æ–‡ä»¶æ•°æ®
    formData.append('audio', {
      uri: fileUri,
      type: 'audio/wav',  // expo-audio-studio é»˜è®¤è¾“å‡ºWAVæ ¼å¼
      name: 'recording.wav',
    } as any);
    
    formData.append('sample_rate', '16000');
    formData.append('enable_vad', 'true');
    formData.append('return_audio', 'false');  // åªéœ€è¦VADç»“æœ
    formData.append('output_format', 'json');

    const response = await fetch(`${API_BASE_URL}/api/v1/mobile/process-audio-efficient`, {
      method: 'POST',
      body: formData,
      headers: {
        'Content-Type': 'multipart/form-data',
      },
    });

    if (!response.ok) {
      throw new Error(`APIè¯·æ±‚å¤±è´¥: ${response.status}`);
    }

    const result = await response.json();
    handleProcessingResult(result);
  };

  // å¤‡ç”¨æ–¹æ³•ï¼šBase64ç¼–ç ï¼ˆå…¼å®¹æ€§æ›´å¥½ï¼‰
  const processWithBase64 = async (fileUri: string) => {
    // è¯»å–éŸ³é¢‘æ–‡ä»¶
    const audioBase64 = await FileSystem.readAsStringAsync(fileUri, {
      encoding: FileSystem.EncodingType.Base64,
    });

    const response = await fetch(`${API_BASE_URL}/api/v1/mobile/process-audio`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        audio_base64: audioBase64,
        format: 'wav',
        sample_rate: 16000,
        enable_vad: true,
        vad_window_duration: 0.5,
        vad_overlap: 0.1,
        return_format: 'segments',
        compress_output: false,
      }),
    });

    if (!response.ok) {
      throw new Error(`APIè¯·æ±‚å¤±è´¥: ${response.status}`);
    }

    const result = await response.json();
    handleProcessingResult(result);
  };

  const handleProcessingResult = (result: any) => {
    if (result.success) {
      if (result.has_speech) {
        const speechDuration = result.speech_segments.reduce(
          (sum: number, seg: any) => sum + seg.duration, 0
        ).toFixed(1);
        
        Alert.alert(
          'å½•éŸ³å¤„ç†æˆåŠŸ',
          `âœ… æ£€æµ‹åˆ°è¯­éŸ³ï¼\n` +
          `ğŸ¤ è¯­éŸ³æ®µæ•°: ${result.speech_segments.length}\n` +
          `â±ï¸ è¯­éŸ³æ—¶é•¿: ${speechDuration}ç§’\n` +
          `ğŸ“Š æ€»æ—¶é•¿: ${result.total_duration}ç§’\n` +
          `ğŸ”„ å¤„ç†æ—¶é—´: ${result.processing_time_ms}ms`,
          [
            {
              text: 'å‘é€åˆ°åç«¯',
              onPress: () => sendToBackend(result),
            },
            { text: 'ç¡®å®š', style: 'default' },
          ]
        );
      } else {
        Alert.alert('æç¤º', 'âŒ æœªæ£€æµ‹åˆ°æœ‰æ•ˆè¯­éŸ³ï¼Œè¯·é‡æ–°å½•åˆ¶');
      }
    } else {
      Alert.alert('å¤„ç†å¤±è´¥', result.message || 'æœªçŸ¥é”™è¯¯');
    }
  };

  const sendToBackend = async (vadResult: any) => {
    try {
      // å‘é€VADç»“æœåˆ°ä½ çš„åç«¯æœåŠ¡
      const response = await fetch(`${API_BASE_URL}/api/your-backend-endpoint`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          audio_file_uri: audioResult?.fileUri,
          vad_result: vadResult,
          timestamp: Date.now(),
          user_id: 'current_user_id', // æ›¿æ¢ä¸ºå®é™…ç”¨æˆ·ID
        }),
      });

      if (response.ok) {
        Alert.alert('æˆåŠŸ', 'âœ… éŸ³é¢‘å·²å‘é€åˆ°åç«¯å¤„ç†');
      } else {
        throw new Error('åç«¯å¤„ç†å¤±è´¥');
      }
    } catch (error) {
      console.error('å‘é€åˆ°åç«¯å¤±è´¥:', error);
      Alert.alert('å‘é€å¤±è´¥', 'âŒ æ— æ³•å‘é€åˆ°åç«¯æœåŠ¡å™¨');
    }
  };

  const playRecording = async () => {
    if (player && audioResult?.fileUri) {
      try {
        await player.play();
      } catch (error) {
        console.error('æ’­æ”¾å¤±è´¥:', error);
        Alert.alert('æ’­æ”¾å¤±è´¥', 'æ— æ³•æ’­æ”¾å½•éŸ³');
      }
    }
  };

  const renderRecordingButton = () => {
    if (isProcessing) {
      return (
        <View style={[styles.recordButton, styles.processingButton]}>
          <Text style={styles.buttonText}>å¤„ç†ä¸­...</Text>
        </View>
      );
    }

    if (isRecording) {
      return (
        <TouchableOpacity
          style={[styles.recordButton, styles.recordingButton]}
          onPress={handleStopRecording}
        >
          <Text style={styles.buttonText}>
            åœæ­¢å½•éŸ³ {(durationMs / 1000).toFixed(1)}s
          </Text>
        </TouchableOpacity>
      );
    }

    if (isPaused) {
      return (
        <View style={styles.pausedContainer}>
          <TouchableOpacity
            style={[styles.recordButton, styles.resumeButton]}
            onPress={resumeRecording}
          >
            <Text style={styles.buttonText}>ç»§ç»­å½•éŸ³</Text>
          </TouchableOpacity>
          <TouchableOpacity
            style={[styles.recordButton, styles.stopButton]}
            onPress={handleStopRecording}
          >
            <Text style={styles.buttonText}>å®Œæˆå½•éŸ³</Text>
          </TouchableOpacity>
        </View>
      );
    }

    return (
      <TouchableOpacity
        style={[styles.recordButton, styles.startButton]}
        onPress={handleStartRecording}
      >
        <Text style={styles.buttonText}>å¼€å§‹å½•éŸ³</Text>
      </TouchableOpacity>
    );
  };

  return (
    <View style={styles.container}>
      <Text style={styles.title}>è¯­éŸ³å½•åˆ¶ä¸VADæ£€æµ‹</Text>
      
      {/* å½•éŸ³çŠ¶æ€æ˜¾ç¤º */}
      {(isRecording || isPaused) && (
        <View style={styles.statusContainer}>
          <Text style={styles.statusText}>
            ğŸ“Š æ—¶é•¿: {(durationMs / 1000).toFixed(1)}ç§’
          </Text>
          <Text style={styles.statusText}>
            ğŸ’¾ å¤§å°: {(size / 1024).toFixed(1)}KB
          </Text>
          {isRecording && (
            <TouchableOpacity
              style={styles.pauseButton}
              onPress={pauseRecording}
            >
              <Text style={styles.pauseButtonText}>æš‚åœ</Text>
            </TouchableOpacity>
          )}
        </View>
      )}

      {/* å½•éŸ³æŒ‰é’® */}
      {renderRecordingButton()}

      {/* æ’­æ”¾æŒ‰é’® */}
      {audioResult?.fileUri && !isRecording && !isPaused && (
        <TouchableOpacity
          style={[styles.recordButton, styles.playButton]}
          onPress={playRecording}
        >
          <Text style={styles.buttonText}>æ’­æ”¾å½•éŸ³</Text>
        </TouchableOpacity>
      )}
    </View>
  );
}

// æ ·å¼å®šä¹‰
const styles = StyleSheet.create({
  container: {
    flex: 1,
    justifyContent: 'center',
    alignItems: 'center',
    padding: 20,
    backgroundColor: '#f5f5f5',
  },
  title: {
    fontSize: 24,
    fontWeight: 'bold',
    marginBottom: 30,
    color: '#333',
  },
  statusContainer: {
    backgroundColor: '#fff',
    padding: 15,
    borderRadius: 10,
    marginBottom: 20,
    shadowColor: '#000',
    shadowOffset: { width: 0, height: 2 },
    shadowOpacity: 0.1,
    shadowRadius: 4,
    elevation: 3,
  },
  statusText: {
    fontSize: 16,
    marginVertical: 2,
    color: '#666',
  },
  recordButton: {
    width: 120,
    height: 120,
    borderRadius: 60,
    justifyContent: 'center',
    alignItems: 'center',
    marginVertical: 10,
    shadowColor: '#000',
    shadowOffset: { width: 0, height: 2 },
    shadowOpacity: 0.2,
    shadowRadius: 4,
    elevation: 5,
  },
  startButton: {
    backgroundColor: '#007AFF',
  },
  recordingButton: {
    backgroundColor: '#FF3B30',
  },
  processingButton: {
    backgroundColor: '#FF9500',
  },
  stopButton: {
    backgroundColor: '#FF3B30',
  },
  resumeButton: {
    backgroundColor: '#34C759',
  },
  playButton: {
    backgroundColor: '#5856D6',
  },
  buttonText: {
    color: 'white',
    fontSize: 16,
    fontWeight: 'bold',
    textAlign: 'center',
  },
  pausedContainer: {
    flexDirection: 'row',
    gap: 15,
  },
  pauseButton: {
    backgroundColor: '#FF9500',
    paddingHorizontal: 15,
    paddingVertical: 5,
    borderRadius: 15,
    marginTop: 10,
  },
  pauseButtonText: {
    color: 'white',
    fontSize: 12,
    fontWeight: 'bold',
  },
});
```

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. éŸ³é¢‘å‹ç¼©

å¦‚æœéŸ³é¢‘æ–‡ä»¶è¾ƒå¤§ï¼Œå¯ä»¥å¯ç”¨å‹ç¼©ï¼š

```javascript
{
  return_format: 'base64',
  compress_output: true,  // å°†ä½¿ç”¨ FLAC æ ¼å¼å‹ç¼©
}
```

### 2. ç¼“å­˜ç­–ç•¥

å¯¹äºé‡å¤çš„éŸ³é¢‘ï¼Œå¯ä»¥åœ¨å®¢æˆ·ç«¯å®ç°ç¼“å­˜ï¼š

```javascript
import AsyncStorage from '@react-native-async-storage/async-storage';

const cacheKey = `audio_${audioHash}`;
const cached = await AsyncStorage.getItem(cacheKey);

if (cached) {
  return JSON.parse(cached);
} else {
  const result = await processAudio(audioBase64);
  await AsyncStorage.setItem(cacheKey, JSON.stringify(result));
  return result;
}
```

### 3. åˆ†å—å¤„ç†

å¯¹äºé•¿éŸ³é¢‘ï¼Œå¯ä»¥åˆ†å—å¤„ç†ï¼š

```javascript
const CHUNK_DURATION = 30; // 30ç§’ä¸€å—

// å°†éŸ³é¢‘åˆ†å‰²æˆå¤šä¸ªå—
const chunks = splitAudioIntoChunks(audioBase64, CHUNK_DURATION);

// æ‰¹é‡å¤„ç†
const formData = new FormData();
chunks.forEach((chunk, i) => {
  formData.append('audio_files', new Blob([chunk]), `chunk_${i}.m4a`);
});

const response = await fetch('/api/v1/mobile/batch-process', {
  method: 'POST',
  body: formData,
});
```

## é”™è¯¯å¤„ç†

```javascript
try {
  const response = await processAudio(audioBase64);
  // å¤„ç†æˆåŠŸ
} catch (error) {
  if (error.response) {
    // æœåŠ¡å™¨è¿”å›é”™è¯¯
    switch (error.response.status) {
      case 400:
        Alert.alert('æ ¼å¼é”™è¯¯', 'éŸ³é¢‘æ ¼å¼ä¸æ”¯æŒ');
        break;
      case 413:
        Alert.alert('æ–‡ä»¶å¤ªå¤§', 'è¯·å½•åˆ¶æ›´çŸ­çš„éŸ³é¢‘');
        break;
      case 500:
        Alert.alert('æœåŠ¡å™¨é”™è¯¯', 'è¯·ç¨åé‡è¯•');
        break;
    }
  } else {
    // ç½‘ç»œé”™è¯¯
    Alert.alert('ç½‘ç»œé”™è¯¯', 'è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥');
  }
}
```

## æœ€ä½³å®è·µ

### 1. å½•éŸ³é…ç½®ä¼˜åŒ–
- **é‡‡æ ·ç‡**ï¼šä½¿ç”¨ 16kHzï¼Œå¹³è¡¡è´¨é‡ä¸æ€§èƒ½
- **å£°é“**ï¼šå•å£°é“ï¼Œå‡å°‘50%æ•°æ®é‡
- **æ ¼å¼é€‰æ‹©**ï¼š
  - æ‰¹é‡å¤„ç†ï¼šä¼˜å…ˆ WAV > FLAC > OGG
  - å®æ—¶æµï¼šFloat32 PCMï¼ˆWebSocketäºŒè¿›åˆ¶ï¼‰
  - ç§»åŠ¨ç«¯ï¼šM4Aï¼ˆExpoé»˜è®¤ï¼‰â†’ æœåŠ¡ç«¯è‡ªåŠ¨è½¬æ¢

### 2. API é€‰æ‹©ç­–ç•¥

#### ç§»åŠ¨APPåœºæ™¯
```javascript
// æ¨èï¼šé«˜æ•ˆæ–‡ä»¶ä¸Šä¼ ï¼ˆæ€§èƒ½æå‡33%ï¼‰
const uploadWithFormData = async (audioUri) => {
  const formData = new FormData();
  formData.append('audio', {
    uri: audioUri,
    type: 'audio/wav',
    name: 'recording.wav'
  });
  
  return fetch('/api/v1/mobile/process-audio-efficient', {
    method: 'POST',
    body: formData
  });
};
```

#### å®æ—¶è¯­éŸ³åœºæ™¯
```javascript
// æ¨èï¼šWebSocketäºŒè¿›åˆ¶æµï¼ˆæ€§èƒ½æå‡228%ï¼‰
const streamProcessor = new BinaryStreamVADProcessor(
  'ws://api.com/api/v1/stream/vad-binary'
);

// å®æ—¶å¤„ç†éŸ³é¢‘å—
microphoneStream.on('data', (audioChunk) => {
  streamProcessor.sendAudioBinary(audioChunk);
});
```

#### æ‰¹é‡åˆ†æåœºæ™¯
```javascript
// æ¨èï¼šæ‰¹é‡å¤„ç†API
const formData = new FormData();
audioFiles.forEach(file => formData.append('audio_files', file));
formData.append('merge_results', 'true');

fetch('/api/v1/mobile/batch-process', {
  method: 'POST',
  body: formData
});
```

### 3. WebSocketæœ€ä½³å®è·µ

#### è¿æ¥ç®¡ç†
```javascript
class RobustWebSocketVAD {
  constructor(url) {
    this.url = url;
    this.reconnectAttempts = 0;
    this.maxReconnectAttempts = 5;
    this.reconnectDelay = 1000; // æŒ‡æ•°é€€é¿
    this.connect();
  }

  connect() {
    this.ws = new WebSocket(this.url);
    
    this.ws.onopen = () => {
      console.log('WebSocketè¿æ¥æˆåŠŸ');
      this.reconnectAttempts = 0;
      this.sendConfig();
    };

    this.ws.onclose = () => {
      if (this.reconnectAttempts < this.maxReconnectAttempts) {
        setTimeout(() => {
          this.reconnectAttempts++;
          this.reconnectDelay *= 2; // æŒ‡æ•°é€€é¿
          this.connect();
        }, this.reconnectDelay);
      }
    };

    this.ws.onerror = (error) => {
      console.error('WebSocketé”™è¯¯:', error);
    };
  }

  sendConfig() {
    this.ws.send(JSON.stringify({
      sample_rate: 16000,
      window_size: 1024
    }));
  }
}
```

#### éŸ³é¢‘ç¼“å†²ç®¡ç†
```javascript
class AudioBufferManager {
  constructor(bufferSize = 4096) {
    this.buffer = new Float32Array(bufferSize);
    this.writePos = 0;
    this.readPos = 0;
  }

  write(audioData) {
    // ç¯å½¢ç¼“å†²åŒºå®ç°
    for (let i = 0; i < audioData.length; i++) {
      this.buffer[this.writePos] = audioData[i];
      this.writePos = (this.writePos + 1) % this.buffer.length;
    }
  }

  read(size) {
    const result = new Float32Array(size);
    for (let i = 0; i < size; i++) {
      result[i] = this.buffer[this.readPos];
      this.readPos = (this.readPos + 1) % this.buffer.length;
    }
    return result;
  }
}
```

### 4. VAD å‚æ•°è°ƒä¼˜

#### ä¸åŒåœºæ™¯çš„å‚æ•°å»ºè®®
```javascript
// ä¼šè®®å½•éŸ³ï¼ˆå¤šäººå¯¹è¯ï¼‰
const meetingConfig = {
  vad_window_duration: 0.3,  // çŸ­çª—å£ï¼Œå¿«é€Ÿå“åº”
  vad_overlap: 0.1,          // å°é‡å ï¼Œå‡å°‘å»¶è¿Ÿ
  threshold: 0.3             // è¾ƒä½é˜ˆå€¼ï¼Œæ•è·è½»å£°
};

// è¯­éŸ³åŠ©æ‰‹ï¼ˆå•äººæ¸…æ™°è¯­éŸ³ï¼‰
const assistantConfig = {
  vad_window_duration: 0.5,  // æ ‡å‡†çª—å£
  vad_overlap: 0.2,          // é€‚ä¸­é‡å 
  threshold: 0.5             // æ ‡å‡†é˜ˆå€¼
};

// å˜ˆæ‚ç¯å¢ƒï¼ˆå™ªéŸ³è¿‡æ»¤ï¼‰
const noisyConfig = {
  vad_window_duration: 0.8,  // é•¿çª—å£ï¼Œç¨³å®šåˆ¤æ–­
  vad_overlap: 0.3,          // å¤§é‡å ï¼Œæé«˜å‡†ç¡®æ€§
  threshold: 0.7             // é«˜é˜ˆå€¼ï¼Œè¿‡æ»¤å™ªéŸ³
};
```

### 5. é”™è¯¯å¤„ç†ä¸æ¢å¤

#### HTTP APIé”™è¯¯å¤„ç†
```javascript
const apiCall = async (url, data, retries = 3) => {
  for (let i = 0; i < retries; i++) {
    try {
      const response = await fetch(url, {
        method: 'POST',
        body: data,
        timeout: 30000 // 30ç§’è¶…æ—¶
      });

      if (response.ok) {
        return await response.json();
      }

      // HTTPé”™è¯¯å¤„ç†
      switch (response.status) {
        case 413: // æ–‡ä»¶è¿‡å¤§
          throw new Error('éŸ³é¢‘æ–‡ä»¶è¿‡å¤§ï¼Œè¯·å½•åˆ¶æ›´çŸ­çš„éŸ³é¢‘');
        case 429: // è¯·æ±‚è¿‡é¢‘
          await sleep(1000 * (i + 1)); // é€€é¿ç­‰å¾…
          continue;
        case 500: // æœåŠ¡å™¨é”™è¯¯
          if (i === retries - 1) throw new Error('æœåŠ¡å™¨æš‚æ—¶ä¸å¯ç”¨');
          continue;
        default:
          throw new Error(`APIé”™è¯¯: ${response.status}`);
      }
    } catch (error) {
      if (i === retries - 1) throw error;
      await sleep(1000 * (i + 1));
    }
  }
};
```

#### WebSocketé”™è¯¯æ¢å¤
```javascript
class ResilientsWebSocket {
  constructor(url) {
    this.url = url;
    this.messageQueue = [];
    this.isConnected = false;
    this.connect();
  }

  send(message) {
    if (this.isConnected) {
      this.ws.send(message);
    } else {
      // æ’é˜Ÿç­‰å¾…é‡è¿
      this.messageQueue.push(message);
    }
  }

  onReconnect() {
    // é‡å‘æ’é˜Ÿçš„æ¶ˆæ¯
    while (this.messageQueue.length > 0) {
      const message = this.messageQueue.shift();
      this.ws.send(message);
    }
  }
}
```

### 6. æ€§èƒ½ç›‘æ§ä¸ä¼˜åŒ–

#### æ€§èƒ½æŒ‡æ ‡æ”¶é›†
```javascript
class PerformanceMonitor {
  constructor() {
    this.metrics = {
      apiCalls: 0,
      totalProcessingTime: 0,
      errors: 0,
      reconnections: 0
    };
  }

  recordAPICall(duration, success) {
    this.metrics.apiCalls++;
    this.metrics.totalProcessingTime += duration;
    if (!success) this.metrics.errors++;
  }

  getAverageProcessingTime() {
    return this.metrics.totalProcessingTime / this.metrics.apiCalls;
  }

  getErrorRate() {
    return this.metrics.errors / this.metrics.apiCalls;
  }
}
```

### 7. èµ„æºç®¡ç†

#### å†…å­˜ä¼˜åŒ–
```javascript
// åŠæ—¶æ¸…ç†å¤§å¯¹è±¡
const processAudio = async (audioData) => {
  try {
    const response = await apiCall(audioData);
    return response;
  } finally {
    // æ¸…ç†å†…å­˜
    audioData = null;
    if (global.gc) global.gc(); // Node.jsç¯å¢ƒ
  }
};

// æµå¼å¤„ç†é¿å…å†…å­˜ç´¯ç§¯
const processAudioStream = async function* (audioStream) {
  for await (const chunk of audioStream) {
    const result = await processChunk(chunk);
    yield result;
    // chunkä¼šè¢«åƒåœ¾å›æ”¶
  }
};
```

### 8. ç”¨æˆ·ä½“éªŒä¼˜åŒ–

#### è¿›åº¦åé¦ˆ
```javascript
const processWithProgress = async (audioFile, onProgress) => {
  const steps = [
    { name: 'ä¸Šä¼ éŸ³é¢‘', weight: 30 },
    { name: 'æ ¼å¼è½¬æ¢', weight: 20 },
    { name: 'VADåˆ†æ', weight: 40 },
    { name: 'ç»“æœç”Ÿæˆ', weight: 10 }
  ];

  let progress = 0;
  for (const step of steps) {
    onProgress({ current: step.name, progress });
    await performStep(step);
    progress += step.weight;
    onProgress({ current: step.name, progress });
  }
};
```

### 9. è°ƒè¯•ä¸æµ‹è¯•

#### APIè°ƒè¯•å·¥å…·
```javascript
const debugAPI = {
  logRequest: (url, data) => {
    console.log(`[API] ${url}`, {
      size: data instanceof FormData ? 'FormData' : JSON.stringify(data).length,
      timestamp: new Date().toISOString()
    });
  },

  logResponse: (response, duration) => {
    console.log(`[API] Response`, {
      status: response.status,
      duration: `${duration}ms`,
      timestamp: new Date().toISOString()
    });
  }
};
```

### 10. éƒ¨ç½²æ£€æŸ¥æ¸…å•

- âœ… **FFmpegå®‰è£…**: æ”¯æŒå¤šç§éŸ³é¢‘æ ¼å¼
- âœ… **TEN-VADé…ç½®**: ç¡®ä¿é«˜è´¨é‡VADæ£€æµ‹
- âœ… **WebSocketæ”¯æŒ**: ä»£ç†æœåŠ¡å™¨é…ç½®æ­£ç¡®
- âœ… **æ–‡ä»¶å¤§å°é™åˆ¶**: è®¾ç½®åˆç†çš„ä¸Šä¼ é™åˆ¶
- âœ… **CORSé…ç½®**: å…è®¸è·¨åŸŸè¯·æ±‚
- âœ… **SSLè¯ä¹¦**: WebSocketéœ€è¦WSSè¿æ¥
- âœ… **ç›‘æ§å‘Šè­¦**: APIå“åº”æ—¶é—´å’Œé”™è¯¯ç‡ç›‘æ§

## æœåŠ¡ç«¯è¦æ±‚

ç¡®ä¿å®‰è£…å¿…è¦çš„ä¾èµ–ï¼š

```bash
pip install soundfile numpy
# å¦‚æœéœ€è¦æ”¯æŒ mp3, m4a ç­‰æ ¼å¼
# Ubuntu/Debian: apt-get install ffmpeg
# macOS: brew install ffmpeg
```

## å¿«é€Ÿå‚è€ƒ

### APIç«¯ç‚¹é€Ÿè§ˆ

| ç«¯ç‚¹ | æ–¹æ³• | ç”¨é€” | æ€§èƒ½ | æ¨èåœºæ™¯ |
|------|------|------|------|----------|
| `/api/v1/mobile/process-audio` | POST | Base64éŸ³é¢‘å¤„ç† | åŸºå‡† | å…¼å®¹æ€§ä¼˜å…ˆ |
| `/api/v1/mobile/process-audio-efficient` | POST | ç›´æ¥æ–‡ä»¶ä¸Šä¼  | **+33%** âš¡ | **ç§»åŠ¨APPæ¨è** |
| `/api/v1/mobile/quick-vad` | POST | å¿«é€ŸVADæ£€æµ‹ | **+50%** âš¡ | é¢„æ£€æµ‹ |
| `/api/v1/mobile/batch-process` | POST | æ‰¹é‡å¤„ç† | **+25%** âš¡ | å¤šæ–‡ä»¶å¤„ç† |
| `/api/v1/stream/vad` | WebSocket | JSONæµå¼VAD | **+130%** âš¡ | å®æ—¶+è°ƒè¯• |
| `/api/v1/stream/vad-binary` | WebSocket | äºŒè¿›åˆ¶æµå¼VAD | **+228%** ğŸš€ | **å®æ—¶éŸ³é¢‘æµ** |

### WebSocketè¿æ¥ç¤ºä¾‹

```javascript
// å¿«é€Ÿè¿æ¥WebSocket VAD
const ws = new WebSocket('ws://localhost:8000/api/v1/stream/vad');

ws.onopen = () => {
  // å‘é€é…ç½®
  ws.send(JSON.stringify({
    type: 'config',
    sample_rate: 16000,
    channels: 1
  }));
};

ws.onmessage = (event) => {
  const result = JSON.parse(event.data);
  if (result.type === 'vad') {
    console.log('æ˜¯å¦è¯´è¯:', result.is_speaking);
    console.log('ç½®ä¿¡åº¦:', result.probability);
  }
};

// å‘é€éŸ³é¢‘æ•°æ®
ws.send(JSON.stringify({
  type: 'audio',
  data: audioFloatArray
}));

// ç»“æŸå¤„ç†
ws.send(JSON.stringify({type: 'end'}));
```

### React Nativeå¿«é€Ÿé›†æˆ

```typescript
// 1. å®‰è£…ä¾èµ–
npm install expo-av expo-file-system

// 2. å½•éŸ³å¹¶å¤„ç†
import { Audio } from 'expo-av';
import * as FileSystem from 'expo-file-system';

const recording = new Audio.Recording();
await recording.prepareToRecordAsync(Audio.RecordingOptionsPresets.HIGH_QUALITY);
await recording.startAsync();

// å½•éŸ³å®Œæˆå
await recording.stopAndUnloadAsync();
const uri = recording.getURI();

// ä½¿ç”¨é«˜æ•ˆAPIå¤„ç†
const formData = new FormData();
formData.append('audio', {
  uri: uri,
  type: 'audio/m4a',
  name: 'recording.m4a'
});

const response = await fetch('/api/v1/mobile/process-audio-efficient', {
  method: 'POST',
  body: formData
});

const result = await response.json();
console.log('VADç»“æœ:', result.has_speech);
```

### çŠ¶æ€æ£€æŸ¥å‘½ä»¤

```bash
# æ£€æŸ¥APIå¥åº·çŠ¶æ€
curl http://localhost:8000/api/v1/mobile/health

# æ£€æŸ¥WebSocketæµçŠ¶æ€
curl http://localhost:8000/api/v1/stream/status

# æµ‹è¯•å¿«é€ŸVAD
curl -X POST http://localhost:8000/api/v1/mobile/quick-vad-file \
  -F "audio=@test.wav" \
  -F "sample_rate=16000"
```

### æ•…éšœæ’é™¤

| é—®é¢˜ | è§£å†³æ–¹æ¡ˆ |
|------|----------|
| `TEN-VAD not available` | æ£€æŸ¥TEN-VADåº“è·¯å¾„å’Œæƒé™ |
| `Format not recognised` | å®‰è£…FFmpegæ”¯æŒæ›´å¤šæ ¼å¼ |
| `WebSocket connection failed` | æ£€æŸ¥é˜²ç«å¢™å’Œä»£ç†è®¾ç½® |
| `413 Request Entity Too Large` | å‡å°æ–‡ä»¶å¤§å°æˆ–è°ƒæ•´æœåŠ¡å™¨é™åˆ¶ |
| `é«˜å»¶è¿Ÿ` | ä½¿ç”¨WebSocketäºŒè¿›åˆ¶æµæˆ–é«˜æ•ˆAPI |

---

ğŸ“š **æ›´å¤šæ–‡æ¡£**:
- [Swagger UIæµ‹è¯•æŒ‡å—](./swagger-testing-guide.md)
- [éŸ³é¢‘ä¼ è¾“ä¼˜åŒ–æ–¹æ¡ˆ](./audio-transmission-optimization.md)
- [FFmpegå®‰è£…æŒ‡å—](./ffmpeg-installation.md)